{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and TF setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import keras \n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from scipy import signal, fftpack\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import UpSampling2D, LeakyReLU, Lambda, Add, Multiply, Activation, Conv2DTranspose\n",
    "from keras.layers import Cropping2D, ZeroPadding2D, Flatten, Subtract\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam\n",
    "from functools import partial\n",
    "from New_Layers import *\n",
    "from keras.layers.merge import _Merge\n",
    "from multiprocessing import Pool\n",
    "import copy \n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import astropy\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "from astropy.io.fits.card import UNDEFINED\n",
    "from astropy.io import fits\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "# GAN Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "    def __init__(self, generator, discriminator, training_scheme, \n",
    "                 generator_kwargs={}, discriminator_kwargs={}, \n",
    "                 generator_training_kwargs={}, discriminator_training_kwargs={}):\n",
    "        \n",
    "        assert training_scheme is not None , \"No training scheme selected!\"\n",
    "        assert isinstance(generator, keras.models.Model), \"Generator is not a model!\"\n",
    "        assert isinstance(discriminator, keras.models.Model), \"Discriminator is not a model!\"\n",
    "        \n",
    "        assert type(generator_kwargs) is dict , \"generator kwargs are not a dictionary!\"\n",
    "        assert type(discriminator_kwargs) is dict , \"discriminator kwargs are not a dictionary!\"\n",
    "        assert type(generator_training_kwargs) is dict , \"discriminator training kwargs are not a dictionary!\"\n",
    "        assert type(discriminator_training_kwargs) is dict , \"generator training kwargs are not a dictionary!\"\n",
    "        \n",
    "        \n",
    "        self._training_scheme = training_scheme\n",
    "\n",
    "        self.__generator_model = generator\n",
    "        self.__discriminator_model = discriminator\n",
    "\n",
    "        self.__discriminator = self._training_scheme.compile_discriminator(self.__generator_model, \n",
    "                                                                           self.__discriminator_model, \n",
    "                                                                           **discriminator_kwargs)\n",
    "        self.__generator = self._training_scheme.compile_generator(self.__generator_model, \n",
    "                                                                   self.__discriminator_model,\n",
    "                                                                   **generator_kwargs)\n",
    "        self.generator_training_kwargs = generator_training_kwargs\n",
    "        self.discriminator_training_kwargs = discriminator_training_kwargs\n",
    "    \n",
    "    def generator_model(self): return self.__generator_model\n",
    "    def discriminator_model(self): return self.__discriminator_model\n",
    "    def generator(self): return self.__generator\n",
    "    def discriminator(self): return self.__discriminator\n",
    "    def summaries(self):\n",
    "        print \"\\n\\n\\nGenerator Summary: \\n\"\n",
    "        self.__generator_model.summary()\n",
    "        plot_model(self.__generator_model, show_shapes=True, to_file='GAN_Generator_Model.png')\n",
    "        \n",
    "        print \"\\n\\n\\nDiscriminator Summary: \\n\"\n",
    "        self.__discriminator_model.summary()\n",
    "        plot_model(self.__discriminator_model, show_shapes=True, to_file='GAN_Discriminator_Model.png')\n",
    "        \n",
    "        print \"\\n\\n\\nGenerator Training Model Summary: \\n\"\n",
    "        self.__generator.summary()\n",
    "        plot_model(self.__generator, show_shapes=True, to_file='GAN_Generator_Training_Model.png')\n",
    "       \n",
    "        print \"\\n\\n\\nDiscriminator Training Model Summary: \\n\"\n",
    "        self.__discriminator.summary()\n",
    "        plot_model(self.__discriminator, show_shapes=True, to_file='GAN_Discriminator_Training_Model.png')\n",
    "   \n",
    "    def fit(self, x, y, verbose=False, shuffle=False, steps=None, epochs=None, steps_per_epoch=None, batch_size=None, \n",
    "            generator_training_multiplier=1, discriminator_training_multiplier=1, \n",
    "            generator_callbacks=[],discriminator_callbacks=[], **kwargs):\n",
    "        self.verbose = verbose\n",
    "        self.callbacks_generator, self.callbacks_discriminator= [], []\n",
    "        self.History = keras.callbacks.History()\n",
    "        self.shuffle = shuffle\n",
    "        assert (steps is None and epochs is not None and steps_per_epoch is not None) or \\\n",
    "               (steps is not None and epochs is None and steps_per_epoch is None), \"please supply either steps OR epochs and steps per epoch\"\n",
    "        \n",
    "        assert batch_size is not None, \"batch size is None, please provide batch size\"\n",
    "        try:\n",
    "            iterator = iter(generator_callbacks)            \n",
    "        except TypeError:\n",
    "            assert False, \"generator callbacks are not iterable!\"\n",
    "        \n",
    "        try:\n",
    "            iterator = iter(discriminator_callbacks)            \n",
    "        except TypeError:\n",
    "            assert False, \"discriminator callbacks are not iterable!\"\n",
    "        \n",
    "        for c in generator_callbacks:\n",
    "            c.set_model(self.__generator)\n",
    "            self.callbacks_generator.append(c)\n",
    "        \n",
    "        for c in discriminator_callbacks:\n",
    "            c.set_model(self.__discriminator)\n",
    "            self.callbacks_discriminator.append(c)\n",
    "        \n",
    "        \n",
    "        for callback in self.callbacks_generator + self.callbacks_discriminator + [self.History]:\n",
    "            callback.on_train_begin()\n",
    "        \n",
    "        \n",
    "        if steps is not None:\n",
    "            for i in tqdm(xrange(steps)):\n",
    "                    temp_loss_discriminator, temp_loss_generator = self.__fit_on_batch(x=x, y=y, step=i, \n",
    "                                                                                       shuffle=self.shuffle,\n",
    "                                                                                       batch_size=batch_size, \n",
    "                                                                                       generator_training_multiplier=generator_training_multiplier,\n",
    "                                                                                       discriminator_training_multiplier=discriminator_training_multiplier)\n",
    "                    loss_discriminator = {('discriminator_'+self.__discriminator.metrics_names[i]):item for i,item in enumerate(temp_loss_discriminator)}\n",
    "                    loss_generator = {('generator_'+self.__generator.metrics_names[i]):item for i,item in enumerate(temp_loss_generator)}\n",
    "                    losses = loss_discriminator.copy().update(loss_generator)\n",
    "                    self.History.on_epoch_end(i,losses) # populate history\n",
    "        elif steps_per_epoch is not None and epochs is not None:\n",
    "            for k in xrange(epochs):\n",
    "                for callback in self.callbacks_generator + self.callbacks_discriminator:\n",
    "                    callback.on_epoch_begin(k)\n",
    "                loss_discriminator, loss_generator = None, None\n",
    "                for i in xrange(steps_per_epoch):\n",
    "                    temp_loss_discriminator, temp_loss_generator = self.__fit_on_batch(x=x, y=y, step=i, epoch=k, \n",
    "                                                                                       shuffle=self.shuffle,\n",
    "                                                                                       steps_per_epoch=steps_per_epoch, \n",
    "                                                                                       batch_size=batch_size,\n",
    "                                                                                       generator_training_multiplier=generator_training_multiplier,\n",
    "                                                                                       discriminator_training_multiplier=discriminator_training_multiplier)\n",
    "\n",
    "                    if loss_discriminator is None: loss_discriminator = temp_loss_discriminator\n",
    "                    elif hasattr(loss_discriminator, '__iter__'): loss_discriminator = [x+y for x,y in zip(loss_discriminator, temp_loss_discriminator)]\n",
    "                    else: loss_discriminator += temp_loss_discriminator\n",
    "                        \n",
    "                    if loss_generator is None: loss_generator = temp_loss_generator\n",
    "                    elif hasattr(loss_generator, '__iter__'): loss_generator = [x+y for x,y in zip(loss_generator, temp_loss_generator)]\n",
    "                    else: loss_generator += temp_loss_generator\n",
    "                \n",
    "                loss_discriminator = [item * 1.0/steps_per_epoch for item in loss_discriminator]\n",
    "                loss_generator = [item * 1.0/steps_per_epoch for item in loss_generator]\n",
    "                                    \n",
    "                for callback in self.callbacks_generator:\n",
    "                    callback.on_epoch_end(k, logs={self.__generator.metrics_names[i]:item for i,item in enumerate(loss_generator)})\n",
    "\n",
    "                for callback in self.callbacks_discriminator:\n",
    "                    callback.on_epoch_end(k, logs={self.__discriminator.metrics_names[i]:item for i,item in enumerate(loss_discriminator)})\n",
    "\n",
    "                loss_discriminator = {('discriminator_'+self.__discriminator.metrics_names[i]):item for i,item in enumerate(loss_discriminator)}\n",
    "                loss_generator = {('generator_'+self.__generator.metrics_names[i]):item for i,item in enumerate(loss_generator)}\n",
    "                losses = loss_discriminator.copy().update(loss_generator)\n",
    "                self.History.on_epoch_end(k,losses) # populate history\n",
    "                    \n",
    "        for callback in self.callbacks_generator + self.callbacks_discriminator:\n",
    "            callback.on_train_end()\n",
    "\n",
    "        return self.History\n",
    "\n",
    "\n",
    "    def __fit_on_batch(self, x, y, step, epoch=None, steps_per_epoch=None, generator_training_multiplier=1, \n",
    "                       discriminator_training_multiplier=1, batch_size=None, shuffle=False, **kwargs):\n",
    "        for callback in self.callbacks_generator + self.callbacks_discriminator:\n",
    "                if epoch is not None:\n",
    "                    callback.on_batch_begin(step, logs={'batch':step, 'size':batch_size})\n",
    "                else:\n",
    "                    callback.on_epoch_begin(step)\n",
    "                    \n",
    "        steps = step if epoch is None else step + (epoch * steps_per_epoch)\n",
    "        curr_x, curr_y = x, y\n",
    "\n",
    "        # Discriminator Training\n",
    "        loss = None\n",
    "        for j in range(discriminator_training_multiplier): \n",
    "            curr = (steps*discriminator_training_multiplier + j) * batch_size\n",
    "            nex = curr + batch_size\n",
    "            idxs = [i % curr_x.shape[0] for i in range(curr,nex)]\n",
    "            if shuffle: idxs = np.random.randint(0, curr_x.shape[0], size=batch_size)\n",
    "            train_x, train_y = curr_x[idxs], curr_y[idxs]\n",
    "            temp_loss = self._training_scheme.train_discriminator(self.__discriminator, train_x, train_y, batch_size, **self.discriminator_training_kwargs)\n",
    "            if loss is None: loss = temp_loss\n",
    "            elif hasattr(loss, '__iter__'): loss = [x+y for x,y in zip(loss, temp_loss)]\n",
    "            else: loss += temp_loss\n",
    "             \n",
    "        loss_discriminator = [item * 1.0/discriminator_training_multiplier for item in loss]\n",
    "        if self.verbose: print 'Discriminator Loss:', loss_discriminator\n",
    "        \n",
    "        # Generator Training\n",
    "        loss = None\n",
    "        for j in range(generator_training_multiplier): \n",
    "            curr = (steps*generator_training_multiplier + j) * batch_size\n",
    "            nex = curr + batch_size\n",
    "            idxs = [i % curr_x.shape[0] for i in range(curr,nex)]\n",
    "            if shuffle: idxs = np.random.randint(0, curr_x.shape[0], size=batch_size)\n",
    "            train_x, train_y = curr_x[idxs], curr_y[idxs]\n",
    "            temp_loss = self._training_scheme.train_generator(self.__generator, train_x, train_y, batch_size, **self.generator_training_kwargs)\n",
    "            if loss is None: loss = temp_loss\n",
    "            elif hasattr(loss, '__iter__'): loss = [x+y for x,y in zip(loss, temp_loss)]\n",
    "            else: loss += temp_loss\n",
    "            \n",
    "        loss_generator = [item * 1.0/generator_training_multiplier for item in loss]\n",
    "        if self.verbose: print 'Generator Loss:', loss_generator\n",
    "\n",
    "        for callback in self.callbacks_generator: # callbacks for generator\n",
    "            if epoch is not None:\n",
    "                callback.on_batch_end(step, logs={self.__generator.metrics_names[i]:item for i,item in enumerate(loss_generator)})\n",
    "            else:\n",
    "                callback.on_epoch_end(step, logs={self.__generator.metrics_names[i]:item for i,item in enumerate(loss_generator)})\n",
    "            \n",
    "        for callback in self.callbacks_discriminator: # callbacks for discriminator\n",
    "            if epoch is not None:\n",
    "                callback.on_batch_end(step, logs={self.__discriminator.metrics_names[i]:item for i,item in enumerate(loss_discriminator)})\n",
    "            else:\n",
    "                callback.on_epoch_end(step, logs={self.__discriminator.metrics_names[i]:item for i,item in enumerate(loss_discriminator)})\n",
    "\n",
    "        return loss_discriminator, loss_generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    " \n",
    "# IWGAN TrainingScheme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_TrainingScheme(object):\n",
    "    @staticmethod\n",
    "    def compile_discriminator(generator, discriminator, **kwargs):\n",
    "            raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_generator(generator, discriminator, **kwargs):\n",
    "            raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    @staticmethod\n",
    "    def train_discriminator(discriminator, x, y, batch_size, **kwargs):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_generator(generator, x, y, batch_size, **kwargs):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "    \n",
    "class IWGAN_TrainingScheme(Base_TrainingScheme):\n",
    "\n",
    "    @staticmethod\n",
    "    def wasserstein_loss(y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_penalty_loss(y_true, y_pred, averaged_samples, gradient_penalty_weight):\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr, axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        gradient_penalty = gradient_penalty_weight * K.square(1 - gradient_l2_norm)\n",
    "        return K.mean(gradient_penalty)\n",
    "\n",
    "    @staticmethod\n",
    "    class RandomWeightedAverage(_Merge):\n",
    "        def __init__(self, batch_size,**kwargs):\n",
    "            super(IWGAN_TrainingScheme.RandomWeightedAverage, self).__init__(**kwargs)\n",
    "            self.batch_size = batch_size\n",
    "            \n",
    "        def _merge_function(self, inputs):\n",
    "            weights = K.random_uniform((self.batch_size, 1, 1, 1))\n",
    "            return (weights * inputs[0]) + ((1 - weights) * inputs[1])\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_discriminator(generator, discriminator, optimizer, batch_size, **kwargs):\n",
    "        gp_weight = 10\n",
    "        for layer in generator.layers: layer.trainable = False\n",
    "        generator.trainable = False\n",
    "        inp = Input(tuple(generator.layers[0].input_shape[1:]))\n",
    "        in_gen = generator(inp)\n",
    "        in_real = Input(tuple(discriminator.layers[0].input_shape[1:]))\n",
    "        discriminator_output_from_generator = discriminator(in_gen)\n",
    "        discriminator_output_from_real_samples = discriminator(in_real)\n",
    "        averaged_samples = IWGAN_TrainingScheme.RandomWeightedAverage(batch_size)([in_real, in_gen])\n",
    "        averaged_samples_out = discriminator(averaged_samples)\n",
    "        partial_gp_loss = partial(IWGAN_TrainingScheme.gradient_penalty_loss, averaged_samples=averaged_samples,\n",
    "                                  gradient_penalty_weight=gp_weight)\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty'\n",
    "        in_gen.trainable = False\n",
    "\n",
    "        # ----- Discriminator -----\n",
    "        discriminator_model = Model(inputs=[in_real, inp], outputs=[discriminator_output_from_real_samples,\n",
    "                                                                    discriminator_output_from_generator,\n",
    "                                                                    averaged_samples_out])\n",
    "        discriminator_model.layers[1].trainable = False\n",
    "        discriminator_model.compile(optimizer=optimizer, loss=[IWGAN_TrainingScheme.wasserstein_loss,\n",
    "                                                               IWGAN_TrainingScheme.wasserstein_loss, partial_gp_loss])\n",
    "        # ----- Discriminator -----\n",
    "\n",
    "        for layer in generator.layers: layer.trainable = True\n",
    "        generator.trainable = True\n",
    "        return discriminator_model\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_generator(generator, discriminator, gen_loss, dis_loss, optimizer, \n",
    "                          gen_metrics, dis_metrics, gen_dis_loss_ratio, **kwargs):\n",
    "\n",
    "        \n",
    "        for layer in discriminator.layers: layer.trainable = False\n",
    "        discriminator.trainable = False\n",
    "        inp = Input(tuple(generator.layers[0].input_shape[1:]))\n",
    "        gen_out = generator(inp)\n",
    "\n",
    "        # ----- Generator -----\n",
    "        model = Model(inp, [gen_out,discriminator(generator(inp))])\n",
    "        model.layers[2].trainable = False\n",
    "        model.compile(loss={'discriminator': dis_loss, 'generator': gen_loss},\n",
    "                      optimizer= optimizer, metrics={'discriminator':dis_metrics, 'generator':gen_metrics},\n",
    "                      loss_weights={'discriminator': 1-gen_dis_loss_ratio, 'generator':gen_dis_loss_ratio})\n",
    "        # ----- Generator -----\n",
    "\n",
    "        for layer in discriminator.layers: layer.trainable = True\n",
    "        discriminator.trainable = True\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def train_discriminator(discriminator, x, y, batch_size, **kwargs):\n",
    "        # Discriminator Training\n",
    "        loss = discriminator.train_on_batch([y, x],  # inp_real, x\n",
    "                                            [np.ones((batch_size, 1), dtype=np.float32),  # discriminator_output_from_real_samples\n",
    "                                            -np.ones((batch_size, 1), dtype=np.float32),  # discriminator_output_from_generator\n",
    "                                            np.zeros((batch_size, 1), dtype=np.float32)])  # averaged_samples_out\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def train_generator(generator, x, y, batch_size, **kwargs):\n",
    "        # Generator Training\n",
    "        loss = generator.train_on_batch(x, [y, np.ones((batch_size, 1), dtype=np.float32)])            #x, [y , 1]\n",
    "                                 \n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAN(generator=Generator(x_train.shape[1:]), \n",
    "            discriminator=Discriminator(x_train.shape[1:]), \n",
    "            training_scheme=IWGAN_TrainingScheme,\n",
    "            generator_kwargs=Generator_kwargs(), \n",
    "            discriminator_kwargs=Discriminator_kwargs(batch_size=32), \n",
    "            generator_training_kwargs={}, \n",
    "            discriminator_training_kwargs={})\n",
    "print 'loaded GAN'\n",
    "#model.summaries()\n",
    "\n",
    "#classifier = compile_classifier(Classifier(), model.generator_model(), 'binary_crossentropy', Adam(), ['accuracy'], None, 'classifier', reverse_freeze=True)\n",
    "#print 'loaded Classifier'\n",
    "#class_train = classifier_training(classifier, 1, 32)\n",
    "log_callback_train = log_results_multi(100, 'best_model_train_resnet_se.txt', network_name='FCN-SE', \n",
    "                                       x=x_train, y=y_train, save_path='best_model_train_resnet_se.h5', image_name='train')\n",
    "log_callback_test = log_results_multi(100, 'best_model_test_resnet_se.txt', network_name='FCN-SE', \n",
    "                                      x=x_test, y=y_test, save_path='best_model_test_resnet_se.h5', image_name='test')\n",
    "def sch(epoch):\n",
    "    if epoch < 750:\n",
    "        return 1e-3\n",
    "    \n",
    "    if epoch < 1500:\n",
    "        if epoch == 750: print(\"changed lr to 5e-5\")\n",
    "        return 5e-5 \n",
    "    \n",
    "    if epoch == 1500: print(\"changed lr to 1e-5\")\n",
    "    return 1e-5\n",
    "import missinglink\n",
    "missinglink_callback_gen = missinglink.KerasCallback()\n",
    "missinglink_callback_gen.set_properties(display_name='generator test')\n",
    "missinglink_callback_dis = missinglink.KerasCallback()    \n",
    "missinglink_callback_dis.set_properties(display_name='discriminator test')\n",
    "schedule = keras.callbacks.LearningRateScheduler(sch)\n",
    "model.fit(x_train, y_train, steps=20001, batch_size=32, shuffle=True,\n",
    "          generator_callbacks=[log_callback_train, log_callback_test, schedule, missinglink_callback_gen],\n",
    "          discriminator_callbacks=[schedule, missinglink_callback_dis])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TF 1.1.0, Keras 2.0.8)",
   "language": "python",
   "name": "tf-1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
